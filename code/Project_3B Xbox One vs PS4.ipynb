{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-0519a017ecd4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-0519a017ecd4>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pip install wordcloud\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud\n",
    "pip install matplotlib_venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maths\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "# visual\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2, venn2_circles\n",
    "%matplotlib inline\n",
    "\n",
    "# modelling\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler,PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score,mean_squared_error,confusion_matrix,accuracy_score\n",
    "from sklearn.pipeline import make_pipeline,Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# nlp\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer,HashingVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "# web\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# others\n",
    "import time\n",
    "import datetime as dt\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Data cleaning & Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordNetLemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2002ef41edcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# clean data using re library and WordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mwnl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WordNetLemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "# clean data using re library and WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    #print(text)    \n",
    "    text = text.lower() # To lowercase    \n",
    "    text = re.sub(r'\\&\\w*;', '', text) # Remove HTML special entities (e.g. &amp;)        \n",
    "    text = re.sub(r'\\s\\s+', ' ', text) # Remove whitespace (including new line characters)        \n",
    "    text = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text) # Remove hyperlinks \n",
    "    #print(text)\n",
    "    \n",
    "    words = text.split()\n",
    "    #print(words)\n",
    "    \n",
    "    words_2 = []\n",
    "\n",
    "    for word in words: \n",
    "        word_2 = wnl.lemmatize(word)\n",
    "        #if word != word_2:\n",
    "            #print(word,word_2)\n",
    "        words_2.append(word_2)      \n",
    "\n",
    "    output = ' '.join(words_2)\n",
    "    #print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read file & assign to pandas dataframes\n",
    "xboxone = pd.read_csv(\"../data/xboxone.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read file & assign to pandas dataframes\n",
    "ps4 = pd.read_csv(\"../data/ps4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data shape\n",
    "xboxone.shape, ps4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data head\n",
    "xboxone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data head\n",
    "ps4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "xboxone = xboxone.drop_duplicates([\"data\"]).reset_index(drop=True)7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "ps4 = ps4.drop_duplicates([\"data\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "xboxone.isnull().sum(), ps4.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data shape again\n",
    "xboxone.shape, ps4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check data types\n",
    "xboxone.dtypes, ps4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BeautifulSoup object on a single reddit   \n",
    "example1 = BeautifulSoup(ps4['data'][0])\n",
    "\n",
    "# Print the raw reddit and then the output of get_text(), for \n",
    "# comparison\n",
    "print(xboxone['data'][1])\n",
    "print()\n",
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the raw reddit and then the output of get_text(), for \n",
    "# comparison\n",
    "print(ps4['data'][1])\n",
    "print()\n",
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expressions to do a find-and-replace\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      example1.get_text())   # The text to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display columns\n",
    "print(xboxone.columns, ps4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display data types\n",
    "xboxone.dtypes, ps4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display data summary\n",
    "xboxone.describe(), ps4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls in columns\n",
    "null_cols = xboxone.isnull().sum()\n",
    "mask_null = null_cols > 0\n",
    "null_cols[mask_null].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls in rows\n",
    "null_rows = xboxone.isnull().sum(axis=1)\n",
    "mask_null = null_rows > 0\n",
    "null_rows[mask_null].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls in columns\n",
    "null_cols = ps4.isnull().sum()\n",
    "mask_null = null_cols > 0\n",
    "null_cols[mask_null].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls in rows\n",
    "null_rows = ps4.isnull().sum(axis=1)\n",
    "mask_null = null_rows > 0\n",
    "null_rows[mask_null].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show column summary\n",
    "xboxone.info(),ps4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text for xboxone and ps4\n",
    "xboxone['data'] = xboxone['data'].apply(clean_text)\n",
    "ps4['data'] = ps4['data'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top words in xboxone\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "cvec.fit(xboxone['data'])\n",
    "\n",
    "df_xboxone = pd.DataFrame(cvec.transform(xboxone['data']).todense(),columns=cvec.get_feature_names())\n",
    "\n",
    "top_xboxone = df_xboxone.sum(axis=0)\n",
    "top_xboxone.sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top words in ps4\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "cvec.fit(ps4['data'])\n",
    "\n",
    "df_ps4 = pd.DataFrame(cvec.transform(ps4['data']).todense(),columns=cvec.get_feature_names())\n",
    "\n",
    "top_ps4 = df_ps4.sum(axis=0)\n",
    "top_ps4.sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top words in xboxone\n",
    "\n",
    "df_xboxone = top_xboxone.to_frame(name='count')\n",
    "df_xboxone['word'] = df_xboxone.index\n",
    "df_xboxone.reset_index(drop=True, inplace=True)\n",
    "df_xboxone['freq'] = df_xboxone['count'] / len(xboxone) * 100\n",
    "\n",
    "cols = ['word','count','freq']\n",
    "df_xboxone = df_xboxone[cols]\n",
    "df_xboxone.sort_values(by='count',ascending=False, inplace=True)\n",
    "df_xboxone.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top words in ps4\n",
    "\n",
    "df_ps4 = top_ps4.to_frame(name='count')\n",
    "df_ps4['word'] = df_ps4.index\n",
    "df_ps4.reset_index(drop=True, inplace=True)\n",
    "df_ps4['freq'] = df_ps4['count'] / len(ps4) * 100\n",
    "\n",
    "cols = ['word','count','freq']\n",
    "df_ps4 = df_ps4[cols]\n",
    "df_ps4.sort_values(by='count',ascending=False, inplace=True)\n",
    "df_ps4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title('xboxone: Top 20 Words',fontsize=25)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.barplot(data=df_xboxone.head(20),x='freq',y='word',orient='h')\n",
    "\n",
    "plt.xlabel('frequency (%)',fontsize=20)\n",
    "plt.ylabel('word',fontsize=20)\n",
    "plt.tick_params(labelsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title('ps4: Top 20 Words',fontsize=25)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.barplot(data=df_ps4.head(20),x='freq',y='word',orient='h')\n",
    "\n",
    "plt.xlabel('frequency (%)',fontsize=20)\n",
    "plt.ylabel('word',fontsize=20)\n",
    "plt.tick_params(labelsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Word Comparsion',fontsize=25)\n",
    "\n",
    "set_a = set(df_xboxone['word'][:20])\n",
    "set_b = set(df_ps4['word'][:20])\n",
    "\n",
    "venn = venn2([set_a,set_b],set_labels=['xboxone','ps4'])\n",
    "\n",
    "venn.get_label_by_id('100').set_text('\\n'.join(map(str,set_a-set_b)))\n",
    "venn.get_label_by_id('110').set_text('\\n'.join(map(str,set_a&set_b)))\n",
    "venn.get_label_by_id('010').set_text('\\n'.join(map(str,set_b-set_a)))\n",
    "\n",
    "venn.get_label_by_id('A').set_size(20)\n",
    "venn.get_label_by_id('B').set_size(20)\n",
    "venn.get_label_by_id('100').set_size(13)\n",
    "venn.get_label_by_id('110').set_size(13)\n",
    "venn.get_label_by_id('010').set_size(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xboxone.isnull().sum(), ps4.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert new column\n",
    "xboxone['console'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check header\n",
    "xboxone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert new column\n",
    "ps4['console'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check header\n",
    "ps4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df by appending xboxone and ps4\n",
    "df = xboxone.append(ps4, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop null values\n",
    "df.dropna(subset=['data'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check header\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_words(string):\n",
    "    df['data'] = df['data'].str.replace(string, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant = ['xboxone',\n",
    "             'xbox',\n",
    "             'ps4',\n",
    "             'false',\n",
    "             'true',\n",
    "             'all_ads',\n",
    "             'approved_at_utc',\n",
    "             'subreddit',\n",
    "             'self',\n",
    "             'text',\n",
    "             'div',\n",
    "             'game',\n",
    "             '39',\n",
    "             'thumbnail_height',\n",
    "             'none',\n",
    "             'flair'\n",
    "            ]\n",
    "for x in redundant:\n",
    "  remove_redundant_words(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(input_string):\n",
    "    tokenizer = RegexpTokenizer(r'[A-Za-z0-9]{2,}')\n",
    "    words = tokenizer.tokenize(input_string)\n",
    "    counter.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word frequency dictionary from the stemmed text for XBox One posts\n",
    "counter = Counter()\n",
    "for input_string in df[df['console']==1]['data']:\n",
    "    word_count(input_string)\n",
    "xboxone_word_freq = dict(counter.most_common())\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(max_font_size=50).generate_from_frequencies(xboxone_word_freq)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud for Xbox One posts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word frequency dictionary from the stemmed text for PS4 posts\n",
    "counter = Counter()\n",
    "for input_string in df[df['console']==0]['data']:\n",
    "    word_count(input_string)\n",
    "ps4_word_freq = dict(counter.most_common())\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(max_font_size=50).generate_from_frequencies(ps4_word_freq)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud for PS4 posts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X (feature matrix) and y (target)\n",
    "\n",
    "X=df[['data']] \n",
    "y=df['console']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.8,test_size=0.2,stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a raw string of text to a string of words\n",
    "def text_processer(raw_text):\n",
    "\n",
    "    #Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    #Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    #Lemmatize words\n",
    "    lemmed_words = [lemm.lemmatize(i) for i in words]\n",
    "    #convert the stop words to a set\n",
    "    stops = set(stopwords.words('english'))\n",
    "    #Remove stop words\n",
    "    meaningful_words = [w for w in lemmed_words if not w in stops]\n",
    "    #Remove redundant words\n",
    "    thing = ['xboxone']\n",
    "    meaningful_words = [w for w in lemmed_words if not w in thing]    \n",
    "    #Join words back into one string separated by space and return result\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of documents based on the dataframe size.\n",
    "data_train = X_train\n",
    "total_train_data = len(data_train)\n",
    "print(f'There are {total_train_data} train documents.')\n",
    "\n",
    "data_test = X_test\n",
    "total_test_data = len(data_test)\n",
    "print(f'There are {total_test_data} test documents.')\n",
    "\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews.\n",
    "clean_train_documents = []\n",
    "clean_test_documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning and parsing the training set documents...\")\n",
    "\n",
    "j = 0\n",
    "for train_documents in data_train['data']:\n",
    "    # Convert review to words, then append to clean_train_reviews.\n",
    "    clean_train_documents.append(text_processer(train_documents))\n",
    "    \n",
    "    # If the index is divisible by 200, print a message\n",
    "    if (j + 1) % 200 == 0:\n",
    "        print(f'Document {j + 1} of {total_train_data}.')\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "print(\"Cleaning and parsing the testing set documents...\")\n",
    "\n",
    "k = 0\n",
    "for test_documents in data_test['data']:\n",
    "    # Convert review to words, then append to clean_train_reviews.\n",
    "    clean_test_documents.append(text_processer(test_documents))\n",
    "    \n",
    "    # If the index is divisible by 200, print a message\n",
    "    if (k + 1) % 200 == 0:\n",
    "        print(f'Review {k + 1} of {total_test_data}.')\n",
    "        \n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check train and test data\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('baseline score:0.523305')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Modeling\n",
    "### 4.1Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check header\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check header\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create baseline model with CountVectorizer\n",
    "cvec = CountVectorizer()\n",
    "cvec.fit(clean_train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvec transform\n",
    "X_train_cvec = cvec.transform(clean_train_documents)\n",
    "X_test_cvec = cvec.transform(clean_test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign df\n",
    "df = pd.DataFrame(X_train_cvec.toarray(),columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use with Logistic Regression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_cvec, y_train)\n",
    "\n",
    "lr_score_train = lr.score(X_train_cvec, y_train)\n",
    "lr_score_test = lr.score(X_test_cvec, y_test)\n",
    "print('lr_score_train:',lr_score_train)\n",
    "print('lr_score_test:',lr_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check length\n",
    "len(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display df\n",
    "df = pd.DataFrame({'Feature': cvec.get_feature_names(), 'log_coef': lr.coef_[0]})\n",
    "df['coef'] = np.exp(df['log_coef'])\n",
    "df['abs_coef'] = abs(df['coef'])\n",
    "df.sort_values('abs_coef', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit naive bayes model\n",
    "model = nb.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = model.predict(X_test_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on training set.\n",
    "model.score(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on testing set.\n",
    "model.score(X_test_cvec, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a confusion matrix.\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print confusion matrix\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9bade583f1b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# confusion matrix metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msensitivity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mspecificity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtn\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tp' is not defined"
     ]
    }
   ],
   "source": [
    "# confusion matrix metrics\n",
    "accuracy = (tp+tn)/(tp+fp+fn+tn)\n",
    "sensitivity = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "precision = tp/(tp+fp)\n",
    "\n",
    "print('Accuracy:',round(accuracy,3))\n",
    "print('Sensitivity:',round(sensitivity,3))\n",
    "print('Specificity:',round(specificity,3))\n",
    "print('Precision:',round(precision,3))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3Logistic Regression Model with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2500, 3000, 3500],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'lr__penalty':['l1','l2'],\n",
    "    'lr__solver':['liblinear'],\n",
    "    'lr__C':np.logspace(-3, 1, 5)\n",
    "}\n",
    "gs_cvec_lr = GridSearchCV(pipe, param_grid=pipe_params, cv=3, verbose=1, n_jobs=-1)\n",
    "gs_cvec_lr.fit(clean_train_documents, y_train)\n",
    "print(gs_cvec_lr.best_score_)\n",
    "gs_cvec_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cvec_lr.score(clean_train_documents,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cvec_lr.score(clean_test_documents,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4MultinomialNB Model with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2500, 3000, 3500],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'mnb__alpha': np.linspace(0.5, 1.5, 3),\n",
    "    'mnb__fit_prior': [True, False],  \n",
    "}\n",
    "gs_cvec_mnb = GridSearchCV(pipe, param_grid=pipe_params, cv=3, verbose=1, n_jobs=-1)\n",
    "gs_cvec_mnb.fit(clean_train_documents, y_train)\n",
    "print(gs_cvec_mnb.best_score_)\n",
    "gs_cvec_mnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cvec_mnb.score(clean_train_documents,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cvec_mnb.score(clean_test_documents,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline score:0.523305\n",
    "\n",
    "Logistic Regression Model with Pipeline training score: 0.9900662251655629\n",
    "\n",
    "Logistic Regression Model with Pipeline testing score: 0.9682539682539683\n",
    "\n",
    "MultinomialNB Model with Pipeline training score: 0.9688741721854305\n",
    "\n",
    "MultinomialNB Model with Pipeline testing score: 0.9629629629629629\n",
    "\n",
    "Based on the best results, Logistic Regression testing with Pipeline scored the better than MultinomialNB. However, both the results are extremely close to each other. Moreover, they are also very close to 1, which might hint at overfitting. Therefore, more data is required to address the overfitting problem. Success will be evaluated based on how much improvement over the baseline score.\n",
    "\n",
    "This project has successfully trained a classifier to differentiate which subreddit the post came from. Consequently, Microsoft can use the classifier to efficiently filter posts from other social media platform for further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
